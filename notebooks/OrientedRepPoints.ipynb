{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Oriented RepPoints\n",
    "\n",
    "* Official code uses MMDetection.\n",
    "* MMDetection models are built from config files - example for [Oriented RepPoints](https://github.com/LiWentomng/OrientedRepPoints/blob/main/configs/dota/orientedreppoints_r50.py).\n",
    "* From this config file, it can be notices the these modules are used:\n",
    "    * [`OrientedRepPointsDetector`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/oriented_detectors/orientedreppoints_detector.py)\n",
    "    * [`OrientedRepPointsHead`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/dense_heads/orientedreppoints_head.py)\n",
    "    * [`OBBPointAssigner`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/core/bbox/assigners/oriented_point_assigner.py)\n",
    "    * [`OBBMaxIoUAssigner`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/core/bbox/assigners/oriented_max_iou_assigner.py)\n",
    "    * [`FocalLoss`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/losses/focal_loss.py)\n",
    "    * [`OBBGIoULoss`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/losses/iou_loss.py)\n",
    "    * [`SpatialBorderLoss`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/losses/spatial_border_loss.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Project's Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from obb.model.oriented_reppoints import OrientedRepPointsHead, rep_point_to_img_space\n",
    "from obb.utils.box_ops import min_area_rect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map P2 with stride  4 have shape: [1, 256, 64, 64]\n",
      "feature map P3 with stride  8 have shape: [1, 256, 32, 32]\n",
      "feature map P4 with stride 16 have shape: [1, 256, 16, 16]\n",
      "feature map P5 with stride 32 have shape: [1, 256, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "img_h, img_w = (256, 256)\n",
    "\n",
    "features_per_map = 256\n",
    "strides = {'P2': 4, 'P3': 8, 'P4': 16, 'P5': 32}\n",
    "feature_maps = {name: torch.rand(1, features_per_map, img_h // stride, img_w // stride)\n",
    "                for name, stride in strides.items()}\n",
    "\n",
    "for name, feature_map in feature_maps.items():\n",
    "    stride = strides[name]\n",
    "    print(f'feature map {name} with stride {strides[name]:2} have shape: {list(feature_map.shape)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Head Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature map P2 with stride = 4:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 64, 64])\n",
      "\tclassification.shape = torch.Size([1, 15, 64, 64])\n",
      "\trep_points1.shape = torch.Size([1, 18, 64, 64])\n",
      "\trep_points2.shape = torch.Size([1, 18, 64, 64])\n",
      "\tob1.shape = torch.Size([1, 64, 64, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 64, 64, 4, 2])\n",
      "\n",
      "feature map P3 with stride = 8:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 32, 32])\n",
      "\tclassification.shape = torch.Size([1, 15, 32, 32])\n",
      "\trep_points1.shape = torch.Size([1, 18, 32, 32])\n",
      "\trep_points2.shape = torch.Size([1, 18, 32, 32])\n",
      "\tob1.shape = torch.Size([1, 32, 32, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 32, 32, 4, 2])\n",
      "\n",
      "feature map P4 with stride = 16:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 16, 16])\n",
      "\tclassification.shape = torch.Size([1, 15, 16, 16])\n",
      "\trep_points1.shape = torch.Size([1, 18, 16, 16])\n",
      "\trep_points2.shape = torch.Size([1, 18, 16, 16])\n",
      "\tob1.shape = torch.Size([1, 16, 16, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 16, 16, 4, 2])\n",
      "\n",
      "feature map P5 with stride = 32:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 8, 8])\n",
      "\tclassification.shape = torch.Size([1, 15, 8, 8])\n",
      "\trep_points1.shape = torch.Size([1, 18, 8, 8])\n",
      "\trep_points2.shape = torch.Size([1, 18, 8, 8])\n",
      "\tob1.shape = torch.Size([1, 8, 8, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 8, 8, 4, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "rotated_RepPoints_head = OrientedRepPointsHead()\n",
    "multi_level_centers1 = []\n",
    "multi_level_centers2 = []\n",
    "for name, feature_map in feature_maps.items():\n",
    "    stride = strides[name]\n",
    "    b = feature_map.shape[0]\n",
    "    h = feature_map.shape[2]\n",
    "    w = feature_map.shape[3]\n",
    "\n",
    "    rep_points1, rep_points2, classification = rotated_RepPoints_head(feature_map)\n",
    "    \n",
    "    # convert rep points to image space in order to calculate the losses\n",
    "    rep_points1 = rep_point_to_img_space(rep_points1, stride)\n",
    "    rep_points2 = rep_point_to_img_space(rep_points2, stride)\n",
    "\n",
    "    # convert rep points out of the model to have shape accepted by conversion function \"g\"\n",
    "    # rep_points1_for_conversion_function = rearrange(rep_points1, 'b (N yx) h w -> b h w N yx', b=1, N=9)\n",
    "    # rep_points2_for_conversion_function = rearrange(rep_points2, 'b (N yx) h w -> b h w N yx', b=1, N=9)\n",
    "\n",
    "    # apply conversion function to get oriented boxes\n",
    "    flattened_ob1 = min_area_rect(rearrange(rep_points1, 'b (N xy) h w -> (b h w) N xy', b=b, N=9))\n",
    "    flattened_ob2 = min_area_rect(rearrange(rep_points2, 'b (N xy) h w -> (b h w) N xy', b=b, N=9))\n",
    "    ob1 = rearrange(flattened_ob1, '(b h w) n xy -> b h w n xy', b=b, h=h, w=w)\n",
    "    ob2 = rearrange(flattened_ob2, '(b h w) n xy -> b h w n xy', b=b, h=h, w=w)\n",
    "\n",
    "    # find rep points centers for assigner\n",
    "    centers1 = rearrange(rep_points1[0, 8:10, :, :], 'xy w h -> (w h) xy')\n",
    "    centers2 = rearrange(rep_points2[0, 8:10, :, :], 'xy w h -> (w h) xy')\n",
    "    stride_vec = torch.ones(centers1.shape[0], 1) * stride\n",
    "    multi_level_centers1.append(torch.cat([centers1, stride_vec], dim=1)) # [..., 3]\n",
    "    multi_level_centers2.append(torch.cat([centers2, stride_vec], dim=1)) # [..., 3]\n",
    "    \n",
    "    print('\\n\\t'.join([\n",
    "        f'\\nfeature map {name} with {stride = }:',\n",
    "        f'{feature_map.shape = }',\n",
    "        f'{classification.shape = }',\n",
    "        f'{rep_points1.shape = }',\n",
    "        f'{rep_points2.shape = }',\n",
    "        f'{ob1.shape = }',\n",
    "        f'{ob2.shape = }',\n",
    "    ]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "rotated_RepPoints_head = OrientedRepPointsHead()\n",
    "\n",
    "multi_level_rep_points1 = {}\n",
    "multi_level_rep_points2 = {}\n",
    "multi_level_classification = {}\n",
    "multi_level_centers1 = {}\n",
    "multi_level_centers2 = {}\n",
    "for name, feature_map in feature_maps.items():\n",
    "    stride = strides[name]\n",
    "\n",
    "    rep_points1, rep_points2, classification = rotated_RepPoints_head(feature_map)\n",
    "\n",
    "    # find rep points centers for assigner\n",
    "    centers1 = rearrange(rep_points1[:, 8:10, :, :], 'b xy w h -> (b w h) xy')\n",
    "    centers2 = rearrange(rep_points2[:, 8:10, :, :], 'b xy w h -> (b w h) xy')\n",
    "    stride_vec = torch.ones(centers1.shape[0], 1) * strides[name]\n",
    "    multi_level_centers1[stride] = torch.cat([centers1, stride_vec], dim=1) # [..., 3]\n",
    "    multi_level_centers2[stride] = torch.cat([centers2, stride_vec], dim=1) # [..., 3]\n",
    "\n",
    "    multi_level_rep_points1[stride] = rearrange(rep_points1, 'b xy w h -> (b w h) xy')\n",
    "    multi_level_rep_points2[stride] = rearrange(rep_points2, 'b xy w h -> (b w h) xy')\n",
    "    multi_level_classification[stride] = rearrange(classification, 'b cls w h -> (b w h) cls')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class OBBPointAssigner:\n",
    "    \"\"\"\n",
    "    Assign a corresponding oriented gt box or background to each point.\n",
    "\n",
    "    Source:\n",
    "        https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/core/bbox/assigners/oriented_point_assigner.py\n",
    "\n",
    "    Each proposals will be assigned with `0`, or a positive integer indicating the ground truth index.\n",
    "    - 0: negative sample, no assigned gt\n",
    "    - positive integer: positive sample, index (1-based) of assigned gt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale=4, pos_num=1):\n",
    "        self.scale = scale\n",
    "        self.pos_num = pos_num\n",
    "        \n",
    "    @staticmethod\n",
    "    def _obb_to_ob(obb_xs, obb_ys):\n",
    "        gt_xmin, _ = obb_xs.min(1)\n",
    "        gt_ymin, _ = obb_ys.min(1)\n",
    "        gt_xmax, _ = obb_xs.max(1)\n",
    "        gt_ymax, _ = obb_ys.max(1)\n",
    "        return torch.cat([gt_xmin[:, None], gt_ymin[:, None], gt_xmax[:, None], gt_ymax[:, None]], dim=1)\n",
    "\n",
    "    def assign(self, points, gt_obboxes, gt_labels=None):\n",
    "        \"\"\"\n",
    "        Assign oriented gt boxes to points.\n",
    "\n",
    "        This method assign a gt obox to every points set, each points set will be assigned with 0, or a positive number.\n",
    "        0 means negative sample, positive number is the index (1-based) of assigned gt.\n",
    "\n",
    "        The assignment is done in following steps (the order matters):\n",
    "            1. assign every points to 0\n",
    "            2. A point is assigned to some gt bbox if:\n",
    "                (i) the point is within the k closest points to the gt bbox\n",
    "                (ii) the distance between this point and the gt is smaller than other gt bboxes\n",
    "        Args:\n",
    "            points (Tensor): points to be assigned, shape(n, 3) while last dimension stands for (x, y, stride).\n",
    "            gt_oboxes (Tensor): Groundtruth oriented boxes, shape (k, 8).\n",
    "            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n",
    "        Returns:\n",
    "            :obj:`AssignResult`: The assign results.\n",
    "        \"\"\"\n",
    "        assert gt_obboxes.size(1) == 8, 'gt_obboxes should be (N * 8)'\n",
    "        \n",
    "        scale = self.scale\n",
    "        num_points = points.shape[0]\n",
    "        num_gts = gt_obboxes.shape[0]\n",
    "\n",
    "        if num_gts == 0 or num_points == 0:\n",
    "            # If no truth assign everything to the background\n",
    "            assigned_gt_inds = points.new_full((num_points,), 0, dtype=torch.long)\n",
    "            if gt_labels is None:\n",
    "                assigned_labels = None\n",
    "            else:\n",
    "                assigned_labels = points.new_zeros((num_points,), dtype=torch.long)\n",
    "            return assigned_gt_inds, assigned_labels\n",
    "\n",
    "        points_xy = points[:, :2]\n",
    "        points_stride = points[:, 2]\n",
    "        points_lvl = torch.log2(points_stride).int()  # [3...,4...,5...,6...,7...]\n",
    "        lvl_min, lvl_max = points_lvl.min(), points_lvl.max()\n",
    "\n",
    "        # assign gt rbox\n",
    "        gt_bboxes = self._obb_to_ob(obb_xs=gt_obboxes[:, 0::2], obb_ys=gt_obboxes[:, 1::2])\n",
    "        gt_bboxes_xy = (gt_bboxes[:, :2] + gt_bboxes[:, 2:]) / 2\n",
    "        gt_bboxes_wh = (gt_bboxes[:, 2:] - gt_bboxes[:, :2]).clamp(min=1e-6)\n",
    "        gt_bboxes_lvl = ((torch.log2(gt_bboxes_wh[:, 0] / scale) + torch.log2(gt_bboxes_wh[:, 1] / scale)) / 2).int()\n",
    "        gt_bboxes_lvl = torch.clamp(gt_bboxes_lvl, min=lvl_min, max=lvl_max)\n",
    "\n",
    "        # stores the assigned gt index of each point\n",
    "        assigned_gt_inds = points.new_zeros((num_points,), dtype=torch.long)\n",
    "\n",
    "        # stores the assigned gt dist (to this point) of each point\n",
    "        assigned_gt_dist = points.new_full((num_points,), float('inf'))\n",
    "        points_range = torch.arange(points.shape[0])\n",
    "\n",
    "        for idx in range(num_gts):\n",
    "            gt_lvl = gt_bboxes_lvl[idx]\n",
    "\n",
    "            # get the index of points in this level\n",
    "            lvl_idx = gt_lvl == points_lvl\n",
    "            points_index = points_range[lvl_idx]\n",
    "\n",
    "            lvl_points = points_xy[lvl_idx, :]\n",
    "            gt_center_point = gt_bboxes_xy[[idx], :]\n",
    "            gt_wh = gt_bboxes_wh[[idx], :]\n",
    "\n",
    "            # compute the distance between gt center and all points in this level\n",
    "            points_gt_dist = ((lvl_points - gt_center_point) / gt_wh).norm(dim=1)\n",
    "\n",
    "            # find the nearest k points to gt center in this level\n",
    "            min_dist, min_dist_index = torch.topk(points_gt_dist, self.pos_num, largest=False)\n",
    "\n",
    "            # the index of nearest k points to gt center in this level\n",
    "            min_dist_points_index = points_index[min_dist_index]\n",
    "\n",
    "            # The less_than_recorded_index stores the index of min_dist that is less then the assigned_gt_dist,\n",
    "            # assigned_gt_dist stores the dist from previous assigned gt (if exist) to each point.\n",
    "            less_than_recorded_index = min_dist < assigned_gt_dist[min_dist_points_index]\n",
    "\n",
    "            # The min_dist_points_index stores the index of points satisfy:\n",
    "            #   (1) it is k nearest to current gt center in this level.\n",
    "            #   (2) it is closer to current gt center than other gt center.\n",
    "            min_dist_points_index = min_dist_points_index[less_than_recorded_index]\n",
    "\n",
    "            # assign the result\n",
    "            assigned_gt_inds[min_dist_points_index] = idx + 1\n",
    "            assigned_gt_dist[min_dist_points_index] = min_dist[less_than_recorded_index]\n",
    "\n",
    "        if gt_labels is not None:\n",
    "            assigned_labels = assigned_gt_inds.new_zeros((num_points,))\n",
    "            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n",
    "            if pos_inds.numel() > 0:\n",
    "                assigned_labels[pos_inds] = gt_labels[assigned_gt_inds[pos_inds] - 1]\n",
    "        else:\n",
    "            assigned_labels = None\n",
    "\n",
    "        return assigned_gt_inds, assigned_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "points1 = torch.cat(list(multi_level_centers1.values()), dim=0)\n",
    "\n",
    "# fake ground truth data [[x1, y1, x2, y2, x3, y3, x4, y4], [...]]\n",
    "gt_obboxes = torch.tensor([[1, 1, 1, 10, 10, 10, 10, 1],\n",
    "                           [10, 10, 10, 50, 50, 50, 50, 10]])\n",
    "gt_labels = torch.tensor([3, 1])\n",
    "\n",
    "assigner = OBBPointAssigner()\n",
    "assigned_gt_inds, assigned_labels = assigner.assign(points1, gt_obboxes, gt_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 389, 5086]),)"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples_idx = torch.where(assigned_labels > 0)\n",
    "positive_samples_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 2])"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assigned_gt_inds[positive_samples_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 1])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assigned_labels[positive_samples_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5.5124,  5.4818,  4.0000],\n        [29.8119, 30.1884,  8.0000]], grad_fn=<IndexBackward>)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples_centers = points1[positive_samples_idx]\n",
    "positive_samples_centers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 4.7541,  3.8993,  5.6108,  5.0836,  5.1042,  5.7450,  6.1869,  3.8524,\n          5.5124,  5.4818,  5.9933,  6.2073,  7.3804,  3.8658,  6.9103,  5.1256,\n          6.9829,  6.1473],\n        [28.9923, 29.3754, 29.4560, 30.0337, 29.0393, 30.7347, 30.1436, 28.9409,\n         29.8119, 30.1884, 30.1215, 31.1001, 31.1416, 28.7015, 31.1061, 30.1200,\n         31.0741, 30.7459]], grad_fn=<IndexBackward>)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_points1 = torch.cat(list(multi_level_rep_points1.values()), dim=0)\n",
    "positive_samples_rep_points1 = rep_points1[positive_samples_idx]\n",
    "positive_samples_rep_points1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/PycharmProjects/OrientedBoundingBox/src/obb/utils/loss.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input).gather(1, target).view(-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(2.7938, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from obb.utils.loss import FocalLoss\n",
    "focal_loss = FocalLoss()\n",
    "\n",
    "# classification loss\n",
    "classification = torch.cat(list(multi_level_classification.values()), dim=0)\n",
    "focal_loss(classification, gt_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}