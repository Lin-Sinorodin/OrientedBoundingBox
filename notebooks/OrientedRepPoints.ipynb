{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Oriented RepPoints\n",
    "\n",
    "* Official code uses MMDetection.\n",
    "* MMDetection models are built from config files - example for [Oriented RepPoints](https://github.com/LiWentomng/OrientedRepPoints/blob/main/configs/dota/orientedreppoints_r50.py).\n",
    "* From this config file, it can be notices the these modules are used:\n",
    "    * [`OrientedRepPointsDetector`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/oriented_detectors/orientedreppoints_detector.py)\n",
    "    * [`OrientedRepPointsHead`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/dense_heads/orientedreppoints_head.py)\n",
    "    * [`OBBPointAssigner`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/core/bbox/assigners/oriented_point_assigner.py)\n",
    "    * [`OBBMaxIoUAssigner`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/core/bbox/assigners/oriented_max_iou_assigner.py)\n",
    "    * [`FocalLoss`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/losses/focal_loss.py)\n",
    "    * [`OBBGIoULoss`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/losses/iou_loss.py)\n",
    "    * [`SpatialBorderLoss`](https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/models/losses/spatial_border_loss.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Project's Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from obb.model.oriented_reppoints import OrientedRepPointsHead, rep_point_to_img_space\n",
    "from obb.utils.box_ops import min_area_rect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map P2 with stride  4 have shape: [1, 256, 64, 64]\n",
      "feature map P3 with stride  8 have shape: [1, 256, 32, 32]\n",
      "feature map P4 with stride 16 have shape: [1, 256, 16, 16]\n",
      "feature map P5 with stride 32 have shape: [1, 256, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "img_h, img_w = (256, 256)\n",
    "\n",
    "features_per_map = 256\n",
    "strides = {'P2': 4, 'P3': 8, 'P4': 16, 'P5': 32}\n",
    "feature_maps = {name: torch.rand(1, features_per_map, img_h // stride, img_w // stride)\n",
    "                for name, stride in strides.items()}\n",
    "\n",
    "for name, feature_map in feature_maps.items():\n",
    "    stride = strides[name]\n",
    "    print(f'feature map {name} with stride {strides[name]:2} have shape: {list(feature_map.shape)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Head Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature map P2 with stride = 4:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 64, 64])\n",
      "\tclassification.shape = torch.Size([1, 15, 64, 64])\n",
      "\trep_points1.shape = torch.Size([1, 18, 64, 64])\n",
      "\trep_points2.shape = torch.Size([1, 18, 64, 64])\n",
      "\tob1.shape = torch.Size([1, 64, 64, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 64, 64, 4, 2])\n",
      "\n",
      "feature map P3 with stride = 8:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 32, 32])\n",
      "\tclassification.shape = torch.Size([1, 15, 32, 32])\n",
      "\trep_points1.shape = torch.Size([1, 18, 32, 32])\n",
      "\trep_points2.shape = torch.Size([1, 18, 32, 32])\n",
      "\tob1.shape = torch.Size([1, 32, 32, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 32, 32, 4, 2])\n",
      "\n",
      "feature map P4 with stride = 16:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 16, 16])\n",
      "\tclassification.shape = torch.Size([1, 15, 16, 16])\n",
      "\trep_points1.shape = torch.Size([1, 18, 16, 16])\n",
      "\trep_points2.shape = torch.Size([1, 18, 16, 16])\n",
      "\tob1.shape = torch.Size([1, 16, 16, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 16, 16, 4, 2])\n",
      "\n",
      "feature map P5 with stride = 32:\n",
      "\tfeature_map.shape = torch.Size([1, 256, 8, 8])\n",
      "\tclassification.shape = torch.Size([1, 15, 8, 8])\n",
      "\trep_points1.shape = torch.Size([1, 18, 8, 8])\n",
      "\trep_points2.shape = torch.Size([1, 18, 8, 8])\n",
      "\tob1.shape = torch.Size([1, 8, 8, 4, 2])\n",
      "\tob2.shape = torch.Size([1, 8, 8, 4, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "rotated_RepPoints_head = OrientedRepPointsHead()\n",
    "multi_level_centers1 = []\n",
    "multi_level_centers2 = []\n",
    "for name, feature_map in feature_maps.items():\n",
    "    stride = strides[name]\n",
    "    h = feature_map.shape[2]\n",
    "    w = feature_map.shape[3]\n",
    "\n",
    "    rep_points1, rep_points2, classification = rotated_RepPoints_head(feature_map)\n",
    "    \n",
    "    # convert rep points to image space in order to calculate the losses\n",
    "    rep_points1 = rep_point_to_img_space(rep_points1, stride)\n",
    "    rep_points2 = rep_point_to_img_space(rep_points2, stride)\n",
    "\n",
    "    # convert rep points out of the model to have shape accepted by conversion function \"g\"\n",
    "    # rep_points1_for_conversion_function = rearrange(rep_points1, 'b (N yx) h w -> b h w N yx', b=1, N=9)\n",
    "    # rep_points2_for_conversion_function = rearrange(rep_points2, 'b (N yx) h w -> b h w N yx', b=1, N=9)\n",
    "\n",
    "    # apply conversion function to get oriented boxes\n",
    "    flattened_ob1 = min_area_rect(rearrange(rep_points1, 'b (N yx) h w -> (b h w) N yx', b=1, N=9))\n",
    "    flattened_ob2 = min_area_rect(rearrange(rep_points2, 'b (N yx) h w -> (b h w) N yx', b=1, N=9))\n",
    "    ob1 = rearrange(flattened_ob1, '(b h w) n yx -> b h w n yx', b=1, h=h, w=w)\n",
    "    ob2 = rearrange(flattened_ob2, '(b h w) n yx -> b h w n yx', b=1, h=h, w=w)\n",
    "\n",
    "    # find rep points centers for assigner\n",
    "    centers1 = rearrange(rep_points1[0, 8:10, :, :], 'yx w h -> (w h) yx')\n",
    "    centers2 = rearrange(rep_points2[0, 8:10, :, :], 'yx w h -> (w h) yx')\n",
    "    stride_vec = torch.ones(centers1.shape[0], 1) * stride\n",
    "    multi_level_centers1.append(torch.cat([centers1, stride_vec], dim=1)) # [..., 3]\n",
    "    multi_level_centers2.append(torch.cat([centers2, stride_vec], dim=1)) # [..., 3]\n",
    "    \n",
    "    print('\\n\\t'.join([\n",
    "        f'\\nfeature map {name} with {stride = }:',\n",
    "        f'{feature_map.shape = }',\n",
    "        f'{classification.shape = }',\n",
    "        f'{rep_points1.shape = }',\n",
    "        f'{rep_points2.shape = }',\n",
    "        f'{ob1.shape = }',\n",
    "        f'{ob2.shape = }',\n",
    "    ]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0, 0, 0,  ..., 0, 0, 0]), tensor([0, 0, 0,  ..., 0, 0, 0]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OBBPointAssigner:\n",
    "    \"\"\"\n",
    "    Assign a corresponding oriented gt box or background to each point.\n",
    "\n",
    "    Source:\n",
    "        https://github.com/LiWentomng/OrientedRepPoints/blob/main/mmdet/core/bbox/assigners/oriented_point_assigner.py\n",
    "\n",
    "    Each proposals will be assigned with `0`, or a positive integer\n",
    "    indicating the ground truth index.\n",
    "    - 0: negative sample, no assigned gt\n",
    "    - positive integer: positive sample, index (1-based) of assigned gt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale=4, pos_num=3):\n",
    "        self.scale = scale\n",
    "        self.pos_num = pos_num\n",
    "\n",
    "    def assign(self, points, gt_obboxes, gt_labels=None):\n",
    "\n",
    "        \"\"\"Assign oriented gt boxes to points.\n",
    "        This method assign a gt obox to every points set, each points set\n",
    "        will be assigned with  0, or a positive number.\n",
    "        0 means negative sample, positive number is the index (1-based) of\n",
    "        assigned gt.\n",
    "        The assignment is done in following steps, the order matters.\n",
    "        1. assign every points to 0\n",
    "        2. A point is assigned to some gt bbox if\n",
    "            (i) the point is within the k closest points to the gt bbox\n",
    "            (ii) the distance between this point and the gt is smaller than\n",
    "                other gt bboxes\n",
    "        Args:\n",
    "            points (Tensor): points to be assigned, shape(n, 3) while last\n",
    "                dimension stands for (x, y, stride).\n",
    "            gt_oboxes (Tensor): Groundtruth oriented boxes, shape (k, 8).\n",
    "            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n",
    "        Returns:\n",
    "            :obj:`AssignResult`: The assign results.\n",
    "        \"\"\"\n",
    "\n",
    "        num_points = points.shape[0]\n",
    "        num_gts = gt_obboxes.shape[0]\n",
    "\n",
    "        if num_gts == 0 or num_points == 0:\n",
    "            # If no truth assign everything to the background\n",
    "            assigned_gt_inds = points.new_full((num_points,), 0, dtype=torch.long)\n",
    "            if gt_labels is None:\n",
    "                assigned_labels = None\n",
    "            else:\n",
    "                assigned_labels = points.new_zeros((num_points,), dtype=torch.long)\n",
    "            return assigned_gt_inds, assigned_labels\n",
    "\n",
    "        points_xy = points[:, :2]\n",
    "        points_stride = points[:, 2]\n",
    "        points_lvl = torch.log2(points_stride).int()  # [3...,4...,5...,6...,7...]\n",
    "        lvl_min, lvl_max = points_lvl.min(), points_lvl.max()\n",
    "\n",
    "        assert gt_obboxes.size(1) == 8, 'gt_obboxes should be (N * 8)'\n",
    "\n",
    "        # gt_obboxes convert to gt_bboxes\n",
    "        gt_xs, gt_ys = gt_obboxes[:, 0::2], gt_obboxes[:, 1::2]\n",
    "        gt_xmin, _ = gt_xs.min(1)\n",
    "        gt_ymin, _ = gt_ys.min(1)\n",
    "        gt_xmax, _ = gt_xs.max(1)\n",
    "        gt_ymax, _ = gt_ys.max(1)\n",
    "        gt_bboxes = torch.cat([gt_xmin[:, None], gt_ymin[:, None],\n",
    "                               gt_xmax[:, None], gt_ymax[:, None]], dim=1)\n",
    "\n",
    "        # assign gt rbox\n",
    "        gt_bboxes_xy = (gt_bboxes[:, :2] + gt_bboxes[:, 2:]) / 2\n",
    "\n",
    "        gt_bboxes_wh = (gt_bboxes[:, 2:] - gt_bboxes[:, :2]).clamp(min=1e-6)\n",
    "        scale = self.scale\n",
    "        gt_bboxes_lvl = ((torch.log2(gt_bboxes_wh[:, 0] / scale) +\n",
    "                          torch.log2(gt_bboxes_wh[:, 1] / scale)) / 2).int()\n",
    "        gt_bboxes_lvl = torch.clamp(gt_bboxes_lvl, min=lvl_min, max=lvl_max)\n",
    "\n",
    "        # stores the assigned gt index of each point\n",
    "        assigned_gt_inds = points.new_zeros((num_points,), dtype=torch.long)\n",
    "\n",
    "        # stores the assigned gt dist (to this point) of each point\n",
    "        assigned_gt_dist = points.new_full((num_points,), float('inf'))\n",
    "        points_range = torch.arange(points.shape[0])\n",
    "\n",
    "        for idx in range(num_gts):\n",
    "            gt_lvl = gt_bboxes_lvl[idx]\n",
    "\n",
    "            # get the index of points in this level\n",
    "            lvl_idx = gt_lvl == points_lvl\n",
    "            points_index = points_range[lvl_idx]\n",
    "\n",
    "            # get the points in this level\n",
    "            lvl_points = points_xy[lvl_idx, :]\n",
    "\n",
    "            # get the center point of gt\n",
    "            gt_point = gt_bboxes_xy[[idx], :]\n",
    "\n",
    "            # get width and height of gt\n",
    "            gt_wh = gt_bboxes_wh[[idx], :]\n",
    "\n",
    "            # compute the distance between gt center and all points in this level\n",
    "            points_gt_dist = ((lvl_points - gt_point) / gt_wh).norm(dim=1)\n",
    "\n",
    "            # find the nearest k points to gt center in this level\n",
    "            min_dist, min_dist_index = torch.topk(points_gt_dist, self.pos_num, largest=False)\n",
    "\n",
    "            # the index of nearest k points to gt center in this level\n",
    "            min_dist_points_index = points_index[min_dist_index]\n",
    "\n",
    "            # The less_than_recorded_index stores the index\n",
    "            #   of min_dist that is less then the assigned_gt_dist. Where\n",
    "            #   assigned_gt_dist stores the dist from previous assigned gt\n",
    "            #   (if exist) to each point.\n",
    "            less_than_recorded_index = min_dist < assigned_gt_dist[min_dist_points_index]\n",
    "\n",
    "            # The min_dist_points_index stores the index of points satisfy:\n",
    "            #   (1) it is k nearest to current gt center in this level.\n",
    "            #   (2) it is closer to current gt center than other gt center.\n",
    "            min_dist_points_index = min_dist_points_index[less_than_recorded_index]\n",
    "\n",
    "            # assign the result\n",
    "            assigned_gt_inds[min_dist_points_index] = idx + 1\n",
    "            assigned_gt_dist[min_dist_points_index] = min_dist[less_than_recorded_index]\n",
    "\n",
    "        if gt_labels is not None:\n",
    "            assigned_labels = assigned_gt_inds.new_zeros((num_points,))\n",
    "            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n",
    "            if pos_inds.numel() > 0:\n",
    "                assigned_labels[pos_inds] = gt_labels[\n",
    "                    assigned_gt_inds[pos_inds] - 1]\n",
    "        else:\n",
    "            assigned_labels = None\n",
    "\n",
    "        return assigned_gt_inds, assigned_labels\n",
    "\n",
    "\n",
    "points1 = torch.cat(multi_level_centers1, dim=0)\n",
    "\n",
    "# fake ground truth data\n",
    "gt_obboxes = torch.tensor([[1, 1, 1, 10, 10, 10, 10, 1]])\n",
    "gt_labels = torch.tensor([1])\n",
    "\n",
    "assigner = OBBPointAssigner()\n",
    "assigner.assign(points1, gt_obboxes, gt_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}